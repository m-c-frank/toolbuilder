use this template. the goal is that the user only supplies the toolname and the directory structure of their respective project and the output will be the <OUTPUT_FILE_FROM_...> you can add to the user message if you need to, for example telling the neuralapi to only return the content of <OUTPUT_FILE_FROM_...>. # toolbuilder üõ†Ô∏è

Empower your development process with `toolbuilder`, the avant-garde solution for tool creation and enhancement. Whether you're a developer seeking to design intricate software tools or a creative enthusiast with a penchant for innovation, `toolbuilder` is here to redefine your crafting journey.

---

## Dive into the world of toolbuilder

`toolbuilder` isn't just another Dockerized solution; it's a vision crafted for developers and creative minds. By harnessing the power of neural API and deploying a Docker-in-Docker approach, we've ensured a platform that supports iterative tool development and optimization. What does this mean for you? A seamless, efficient, and enriched user experience throughout your tool's lifecycle.

## Features üåü

### 1. Iterative Tool Development üîÑ

Benefit from a feedback-driven development process to continuously refine and upgrade your tool functionalities.

### 2. Neural API Integration üß†

Tap into the advanced neural models for genuine, human-like interactions and guidance. Fetch content, understand contexts, and engage in organic conversations.

```python
import os
import openai as neuralapi
from tree_of_thoughts import OpenAILanguageModel, MonteCarloTreeofThoughts

API_KEY = os.environ.get("NEURAL_API_KEY")
API_MODEL = "gpt-3.5-turbo"
NUM_THOUGHTS = 1
MAX_STEPS = 3
MAX_STATES = 4
PRUNING_THRESHOLD = 0.5
PROMPT_TEMPLATE_FILEPATH = "toolbuilder/templates/file_retrieval.ppt"

FUNCTION_CONTEXTS = {
    "fetch_repository_content": PROMPT_TEMPLATE_FILEPATH,
    "select_search_algorithm": "You are the algorithm strategist entity. Reflecting on our earlier interactions, deduce the best search algorithm suited for the presented context.",
    "craft_prompt": "You are the prompt artisan entity. Leveraging your understanding of the context and the chosen algorithm, design a compelling and effective prompt for the Tree of Thoughts algorithm.",
    "general_request": "You are a generalist AI, well-versed in multiple domains. Address the query with accurate and detailed information.",
    "analyze_content": "You are an expert in analyzing Python code. Examine the following content and provide insights.",
    "debug_content": "You are a debugging expert. Analyze the following Python code and provide debugging information.",
    "recommend_optimizations": "You are an optimization specialist. Review the following Python code and provide recommendations for optimization."
}


def get_analysis(content):
    return send_request(content, "analyze_content")

def get_debugging_info(content):
    return send_request(content, "debug_content")

def get_recommendations(content):
    return send_request(content, "recommend_optimizations")

def load_prompt_template(prompt_path):
    with open(prompt_path, 'r') as file:
        return file.read()

def process_template(template_content, dir_tree, requested_file_path):
    return template_content.replace('<REPO_DIR_TREE>', dir_tree).replace('<REQUESTED_FILENAME>', requested_file_path)

def send_request(request_msg, function_name, processed_template=None):
    context_source = FUNCTION_CONTEXTS.get(function_name, FUNCTION_CONTEXTS["general_request"])

    if function_name == "fetch_repository_content":
        context_msg = processed_template
    elif context_source.endswith('.ppt'):
        context_msg = load_prompt_template(context_source)
    else:
        context_msg = context_source

    response = neuralapi.ChatCompletion.create(
        model=API_MODEL,
        messages=[
            {"role": "system", "content": context_msg},
            {"role": "user", "content": request_msg},
        ],
    )
    return response.choices[0].message["content"]

def fetch_repository_content(tool_name, dir_tree, target_file_name):
    template_content = load_prompt_template(PROMPT_TEMPLATE_FILEPATH)
    processed_template = process_template(template_content, dir_tree, target_file_name)
    request_msg = f"{tool_name} | {target_file_name}"
    return send_request(request_msg, "fetch_repository_content", processed_template)

def select_search_algorithm(context):
    algorithms = ["BFS", "DFS", "Best-First", "A*", "MCTS"]
    request_msg = f"Considering our past engagements, which search algorithm from the list {algorithms} would best address the problem context: '{context}'?"
    response = send_request(request_msg, "select_search_algorithm")
    selected_algo = next((algo for algo in algorithms if algo in response), None)
    return selected_algo

def craft_prompt(context, query, algo):
    request_msg = (
        f"Drawing from our previous conversations and your understanding of {algo} within the Tree of Thoughts framework, "
        f"craft a prompt that would navigate the context: '{context}' to address the question: {query}"
    )
    return send_request(request_msg, "craft_prompt")

def iterative_solution(context, query):
    model = OpenAILanguageModel(api_key=API_KEY, api_model=API_MODEL)
    tree_of_thoughts = MonteCarloTreeofThoughts(model)

    algo = select_search_algorithm(context)
    prompt = craft_prompt(context, query, algo)
    solution = tree_of_thoughts.solve(
        initial_prompt=prompt,
        num_thoughts=NUM_THOUGHTS,
        max_steps=MAX_STEPS,
        max_states=MAX_STATES,
        pruning_threshold=PRUNING_THRESHOLD,
    )
    return solution

if __name__ == "__main__":
    dir_tree = "<REPO_DIR_TREE>"
    requested_file_path = "<REQUESTED_FILENAME>"

    content = fetch_repository_content(
        "toolbuilder", dir_tree, requested_file_path
    )
    print("Fetched file content:\n", content)

    context = input("Context: ")
    query = input("Query: ")
    response = iterative_solution(context, query)
    print("\nFinal Response:", response)
```

### Example Output

```python
<OUTPUT_FILE_FROM_NEURALAPI>
```

### 3. Enhanced CLI üñ•Ô∏è

Interact via an intuitive command-line interface, perfect for tool creation, tweaks, and user queries.

### 4. Safety Through Containers üõ°Ô∏è

Experience the Docker-in-Docker setup, ensuring both consistency in tool behavior and robust security.

### 5. Iterative Response Refinement üìù

Work hand-in-hand with the AI. Every response from the neural API can be opened in an editor, like `vim`, for on-the-spot modifications. Once you're satisfied with the edits, simply exit the editor to let the program seamlessly continue its operations. Experience the luxury of real-time adjustments and tailor the AI outputs precisely to your needs.

---

## Getting Started

**Peek Inside - Directory Structure**:

```bash
<REPO_DIR_TREE>
```

do it purely with the given prompt template. the neuralapi will then give the code without any more context.