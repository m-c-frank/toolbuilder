use this template. the goal is that the user only supplies the toolname and the directory structure of their respective project and the output will be the <OUTPUT_FILE_FROM_...> you can add to the user message if you need to, for example telling the neuralapi to only return the content of <OUTPUT_FILE_FROM_...>. # toolbuilder üõ†Ô∏è

Empower your development process with `toolbuilder`, the avant-garde solution for tool creation and enhancement. Whether you're a developer seeking to design intricate software tools or a creative enthusiast with a penchant for innovation, `toolbuilder` is here to redefine your crafting journey.

---

## Dive into the world of toolbuilder

`toolbuilder` isn't just another Dockerized solution; it's a vision crafted for developers and creative minds. By harnessing the power of neural API and deploying a Docker-in-Docker approach, we've ensured a platform that supports iterative tool development and optimization. What does this mean for you? A seamless, efficient, and enriched user experience throughout your tool's lifecycle.

## Features üåü

### 1. Iterative Tool Development üîÑ

Benefit from a feedback-driven development process to continuously refine and upgrade your tool functionalities.

### 2. Neural API Integration üß†

Tap into the advanced neural models for genuine, human-like interactions and guidance. Fetch content, understand contexts, and engage in organic conversations.

```python
import os
import openai as neuralapi
from tree_of_thoughts import OpenAILanguageModel, MonteCarloTreeofThoughts
from toolbuilder.prompt_template_utils import insert_api_content_into_template, load_file_content

# Constants for Configuration
API_KEY = os.environ.get("NEURAL_API_KEY")
API_MODEL = "gpt-3.5-turbo"
NUM_THOUGHTS = 1
MAX_STEPS = 3
MAX_STATES = 4
PRUNING_THRESHOLD = 0.5
PROMPT_TEMPLATE_FILEPATH = "toolbuilder/templates/file_retrieval.ppt"

FUNCTION_CONTEXTS = {
    "fetch_repository_content": PROMPT_TEMPLATE_FILEPATH,
    "select_search_algorithm": "You are the algorithm strategist entity. Reflecting on our earlier interactions, deduce the best search algorithm suited for the presented context.",
    "craft_prompt": "You are the prompt artisan entity. Leveraging your understanding of the context and the chosen algorithm, design a compelling and effective prompt for the Tree of Thoughts algorithm.",
    "general_request": "You are a generalist AI, well-versed in multiple domains. Address the query with accurate and detailed information.",
}

def send_request(request_msg, function_name):
    context_source = FUNCTION_CONTEXTS.get(function_name, FUNCTION_CONTEXTS["general_request"])

    # Determine if the context source is a file path or a hardcoded string
    if context_source.endswith('.ppt'):
        context_msg = load_file_content(context_source)
    else:
        context_msg = context_source

    response = neuralapi.ChatCompletion.create(
        model=API_MODEL,
        messages=[
            {"role": "system", "content": context_msg},
            {"role": "user", "content": request_msg},
        ],
    )
    return response.choices[0].message["content"]

def fetch_repository_content(tool_name, readme_path, target_file_name):
    request_msg = f"{tool_name} | {readme_path} | {target_file_name}"
    return send_request(request_msg, "fetch_repository_content")

def select_search_algorithm(context):
    algorithms = ["BFS", "DFS", "Best-First", "A*", "MCTS"]
    request_msg = f"Considering our past engagements, which search algorithm from the list {algorithms} would best address the problem context: '{context}'?"
    response = send_request(request_msg, "select_search_algorithm")
    selected_algo = next((algo for algo in algorithms if algo in response), None)
    return selected_algo

def craft_prompt(context, query, algo):
    request_msg = (
        f"Drawing from our previous conversations and your understanding of {algo} within the Tree of Thoughts framework, "
        f"craft a prompt that would navigate the context: '{context}' to address the question: {query}"
    )
    return send_request(request_msg, "craft_prompt")

def iterative_solution(context, query):
    model = OpenAILanguageModel(api_key=API_KEY, api_model=API_MODEL)
    tree_of_thoughts = MonteCarloTreeofThoughts(model)

    algo = select_search_algorithm(context)
    prompt = craft_prompt(context, query, algo)
    solution = tree_of_thoughts.solve(
        initial_prompt=prompt,
        num_thoughts=NUM_THOUGHTS,
        max_steps=MAX_STEPS,
        max_states=MAX_STATES,
        pruning_threshold=PRUNING_THRESHOLD,
    )
    return solution

if __name__ == "__main__":
    content = fetch_repository_content(
        "toolbuilder", "./README.md", "./toolbuilder/cli_tool.py"
    )
    formatted_content = insert_api_content_into_template(content, PROMPT_TEMPLATE_FILEPATH)
    print("Fetched and formatted content:\n", formatted_content)

    context = input("Context: ")
    query = input("Query: ")
    response = iterative_solution(context, query)
    print("\nFinal Response:", response)
```

### Example Output

```python
<OUTPUT_FILE_FROM_NEURALAPI>
```

### 3. Enhanced CLI üñ•Ô∏è

Interact via an intuitive command-line interface, perfect for tool creation, tweaks, and user queries.

### 4. Safety Through Containers üõ°Ô∏è

Experience the Docker-in-Docker setup, ensuring both consistency in tool behavior and robust security.

### 5. Iterative Response Refinement üìù

Work hand-in-hand with the AI. Every response from the neural API can be opened in an editor, like `vim`, for on-the-spot modifications. Once you're satisfied with the edits, simply exit the editor to let the program seamlessly continue its operations. Experience the luxury of real-time adjustments and tailor the AI outputs precisely to your needs.

---

## Getting Started

**Peek Inside - Directory Structure**:

```bash
<REPO_DIR_TREE>
```

do it purely with the given prompt template. the neuralapi will then give the code without any more context.